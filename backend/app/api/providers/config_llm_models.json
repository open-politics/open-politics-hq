{
  "providers": {
    "gemini": {
      "name": "Google Gemini",
      "type": "cloud",
      "description": "Google's Gemini models for text generation and analysis",
      "api_key_env": "GOOGLE_API_KEY",
      "base_url": "https://generativelanguage.googleapis.com/v1beta",
      "models": {
        "gemini-2.5-flash-preview-05-20": {
          "name": "gemini-2.5-flash-preview-05-20",
          "description": "Latest Gemini model with reasoning, multimodal capabilities, and structured output",
          "max_tokens": 8192,
          "context_length": 1048576,
          "recommended": true,
          "tags": ["latest", "multimodal", "reasoning", "thinking", "structured"],
          "cost_per_1k_input_tokens": 0.00015,
          "cost_per_1k_output_tokens": 0.0006,
          "supports_multimodal": true,
          "supports_structured_output": true,
          "supports_thinking": true,
          "use_cases": ["rag", "classification", "complex_analysis", "multimodal_understanding"]
        },
        "gemini-2.5-flash-lite-preview-06-17": {
          "name": "gemini-2.5-flash-lite-preview-06-17",
          "description": "Gemini 2.5 Flash-Lite Preview 06-17",
          "max_tokens": 8192,
          "context_length": 1048576,
          "recommended": true,
          "tags": ["latest", "multimodal", "reasoning", "thinking", "structured"],
          "cost_per_1k_input_tokens": 0.00015,
          "cost_per_1k_output_tokens": 0.0006,
          "supports_multimodal": true,
          "supports_structured_output": true,
          "supports_thinking": true,
          "use_cases": ["rag", "classification", "complex_analysis", "multimodal_understanding"]
        }
      }
    },
    "openai": {
      "name": "OpenAI",
      "type": "cloud",
      "description": "OpenAI's GPT models for generation and analysis",
      "api_key_env": "OPENAI_API_KEY",
      "base_url": "https://api.openai.com/v1",
      "models": {
        "gpt-4o": {
          "name": "gpt-4o",
          "description": "Most advanced GPT-4 model with multimodal capabilities",
          "max_tokens": 4096,
          "context_length": 128000,
          "recommended": true,
          "tags": ["multimodal", "advanced", "reasoning"],
          "cost_per_1k_input_tokens": 0.005,
          "cost_per_1k_output_tokens": 0.015,
          "supports_multimodal": true,
          "supports_structured_output": true,
          "use_cases": ["complex_reasoning", "multimodal_analysis", "high_quality_rag"]
        },
        "gpt-4o-mini": {
          "name": "gpt-4o-mini",
          "description": "Efficient GPT-4 model for faster inference",
          "max_tokens": 16384,
          "context_length": 128000,
          "recommended": true,
          "tags": ["efficient", "fast", "cost-effective"],
          "cost_per_1k_input_tokens": 0.00015,
          "cost_per_1k_output_tokens": 0.0006,
          "supports_multimodal": true,
          "supports_structured_output": true,
          "use_cases": ["classification", "fast_rag", "routine_analysis"]
        },
        "gpt-4-turbo": {
          "name": "gpt-4-turbo",
          "description": "High-performance GPT-4 model for complex tasks",
          "max_tokens": 4096,
          "context_length": 128000,
          "recommended": false,
          "tags": ["high-performance", "large-context"],
          "cost_per_1k_input_tokens": 0.01,
          "cost_per_1k_output_tokens": 0.03,
          "supports_multimodal": true,
          "supports_structured_output": true,
          "use_cases": ["complex_analysis", "long_context_rag"]
        },
        "gpt-3.5-turbo": {
          "name": "gpt-3.5-turbo",
          "description": "Fast and cost-effective model for simple tasks",
          "max_tokens": 4096,
          "context_length": 16385,
          "recommended": false,
          "tags": ["fast", "cost-effective", "legacy"],
          "cost_per_1k_input_tokens": 0.0005,
          "cost_per_1k_output_tokens": 0.0015,
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["simple_classification", "basic_rag", "cost_sensitive"]
        }
      }
    },
    "ollama": {
      "name": "Ollama",
      "type": "local",
      "description": "Local LLM models via Ollama",
      "base_url_env": "OLLAMA_BASE_URL",
      "default_base_url": "http://localhost:11434",
      "models": {
        "llama3.1:8b": {
          "name": "llama3.1:8b",
          "description": "Llama 3.1 8B parameter model, good balance of capability and speed",
          "max_tokens": 4096,
          "context_length": 131072,
          "recommended": true,
          "tags": ["open-source", "balanced", "local"],
          "model_size": "4.7GB",
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["local_rag", "classification", "privacy_sensitive"]
        },
        "llama3.1:70b": {
          "name": "llama3.1:70b",
          "description": "Llama 3.1 70B parameter model, high capability",
          "max_tokens": 4096,
          "context_length": 131072,
          "recommended": false,
          "tags": ["open-source", "high-capability", "large"],
          "model_size": "40GB",
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["complex_local_analysis", "high_quality_local_rag"]
        },
        "llama3.2:3b": {
          "name": "llama3.2:3b",
          "description": "Llama 3.2 3B parameter model, fast and efficient",
          "max_tokens": 2048,
          "context_length": 131072,
          "recommended": true,
          "tags": ["fast", "efficient", "small"],
          "model_size": "2.0GB",
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["fast_local_classification", "edge_deployment"]
        },
        "mistral:7b": {
          "name": "mistral:7b",
          "description": "Mistral 7B model, efficient and capable",
          "max_tokens": 4096,
          "context_length": 32768,
          "recommended": true,
          "tags": ["efficient", "open-source", "mistral"],
          "model_size": "4.1GB",
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["local_analysis", "structured_extraction"]
        },
        "mixtral:8x7b": {
          "name": "mixtral:8x7b",
          "description": "Mixtral 8x7B mixture of experts model",
          "max_tokens": 4096,
          "context_length": 32768,
          "recommended": false,
          "tags": ["moe", "high-capability", "large"],
          "model_size": "26GB",
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["complex_local_reasoning", "expert_analysis"]
        },
        "phi3:mini": {
          "name": "phi3:mini",
          "description": "Microsoft Phi-3 Mini, small but capable model",
          "max_tokens": 2048,
          "context_length": 131072,
          "recommended": true,
          "tags": ["small", "efficient", "microsoft"],
          "model_size": "2.3GB",
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["resource_constrained", "fast_inference"]
        }
      }
    },
    "anthropic": {
      "name": "Anthropic",
      "type": "cloud",
      "description": "Anthropic's Claude models for analysis and generation",
      "api_key_env": "ANTHROPIC_API_KEY",
      "base_url": "https://api.anthropic.com/v1",
      "models": {
        "claude-3-5-sonnet": {
          "name": "claude-3-5-sonnet",
          "description": "Most capable Claude model with excellent reasoning",
          "max_tokens": 8192,
          "context_length": 200000,
          "recommended": true,
          "tags": ["advanced", "reasoning", "large-context"],
          "cost_per_1k_input_tokens": 0.003,
          "cost_per_1k_output_tokens": 0.015,
          "supports_multimodal": true,
          "supports_structured_output": true,
          "use_cases": ["complex_analysis", "long_context_rag", "detailed_reasoning"]
        },
        "claude-3-haiku": {
          "name": "claude-3-haiku",
          "description": "Fast and efficient Claude model",
          "max_tokens": 4096,
          "context_length": 200000,
          "recommended": true,
          "tags": ["fast", "efficient", "cost-effective"],
          "cost_per_1k_input_tokens": 0.00025,
          "cost_per_1k_output_tokens": 0.00125,
          "supports_multimodal": false,
          "supports_structured_output": true,
          "use_cases": ["fast_classification", "efficient_rag", "quick_analysis"]
        }
      }
    }
  },
  "default_providers": {
    "development": "ollama",
    "production": "gemini",
    "local": "ollama",
    "cloud": "gemini"
  },
  "recommended_models": {
    "classification": ["gemini-2.5-flash-preview-05-20", "gpt-4o-mini", "llama3.1:8b", "claude-3-haiku"],
    "rag": ["gemini-2.5-flash-preview-05-20", "gpt-4o", "llama3.1:8b", "claude-3-5-sonnet"],
    "complex_reasoning": ["gemini-2.5-flash-preview-05-20", "gpt-4o", "claude-3-5-sonnet", "llama3.1:70b"],
    "fast_inference": ["gemini-2.5-flash-preview-05-20", "gpt-4o-mini", "llama3.2:3b", "phi3:mini"],
    "cost_effective": ["gemini-2.5-flash-preview-05-20", "gpt-4o-mini", "claude-3-haiku", "llama3.1:8b"],
    "local_deployment": ["llama3.1:8b", "mistral:7b", "phi3:mini", "llama3.2:3b"],
    "multimodal": ["gemini-2.5-flash-preview-05-20", "gpt-4o", "claude-3-5-sonnet", "gpt-4o-mini"],
    "long_context": ["claude-3-5-sonnet", "claude-3-haiku", "llama3.1:8b", "gpt-4-turbo"],
    "thinking": ["gemini-2.5-flash-preview-05-20", "claude-3-5-sonnet", "gpt-4o"]
  },
  "use_case_defaults": {
    "rag_adapter": {
      "development": "llama3.1:8b",
      "production": "gemini-2.5-flash-preview-05-20"
    },
    "classification_adapter": {
      "development": "llama3.2:3b",
      "production": "gemini-2.5-flash-preview-05-20"
    },
    "analysis_adapter": {
      "development": "mistral:7b",
      "production": "gemini-2.5-flash-preview-05-20"
    }
  }
} 