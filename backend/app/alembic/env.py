import os
from logging.config import fileConfig

from alembic import context
from sqlalchemy import engine_from_config, pool
from alembic.operations import ops


# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)

# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
# target_metadata = None

from app.models import SQLModel  # noqa

# --- ADD THIS SECTION ---
from sqlalchemy_celery_beat.models import ModelBase as CeleryBeatModelBase # Import the base

# Combine your app's metadata with the Celery Beat metadata
target_metadata = [SQLModel.metadata, CeleryBeatModelBase.metadata]
# --- END OF ADDED SECTION ---

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_url():
    user = os.getenv("POSTGRES_USER", "postgres")
    password = os.getenv("POSTGRES_PASSWORD", "")
    server = os.getenv("POSTGRES_SERVER", "db")
    port = os.getenv("POSTGRES_PORT", "5432")
    db = os.getenv("POSTGRES_DB", "app")
    return f"postgresql+psycopg://{user}:{password}@{server}:{port}/{db}"


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        compare_type=True,
        include_schemas=True,
        process_revision_directives=process_revision_directives
    )

    with context.begin_transaction():
        context.run_migrations()


def process_revision_directives(context, revision, directives):
    """
    Inspect autogenerated directives and prepend CREATE SCHEMA statements
    if tables are being created in non-default schemas.
    """
    # There's usually only one directive for a single --autogenerate command
    for directive in directives:
        # Check if the directive has upgrade_ops and if it's not empty
        # This avoids importing MigrationScript directly
        if hasattr(directive, 'upgrade_ops') and directive.upgrade_ops and directive.upgrade_ops.ops:
            upgrade_ops = directive.upgrade_ops.ops
            schemas = set()
            # Iterate through top-level operations/containers
            for op_container in upgrade_ops: 
                 # Handle ops directly within upgrade_ops or inside containers
                 actual_ops = []
                 if hasattr(op_container, 'ops'): # Check if it's a container like ModifyTableOps
                      actual_ops.extend(op_container.ops)
                 else: # Assume it's a direct operation
                      actual_ops.append(op_container)

                 for op in actual_ops:
                      # Check if it's a CreateTableOp and has a schema defined
                      if isinstance(op, ops.CreateTableOp) and op.schema:
                           schemas.add(op.schema)

            # Prepend CREATE SCHEMA for each detected schema if it doesn't exist
            # Insert in reverse order of discovery to maintain relative order if needed,
            # though CREATE SCHEMA order usually doesn't matter.
            # Inserting at index 0 prepends the operations.
            for schema in sorted(list(schemas), reverse=True):
                 print(f"Detected schema '{schema}' in CreateTableOp, prepending CREATE SCHEMA")
                 # Create the execute operation for schema creation
                 create_schema_op = ops.ExecuteSQLOp(f"CREATE SCHEMA IF NOT EXISTS {schema}")
                 # Insert the operation at the beginning of the main upgrade script list
                 directive.upgrade_ops.ops.insert(0, create_schema_op)
    # Note: We are intentionally NOT adding DROP SCHEMA to downgrade directives
    # for safety, as per the recommendation.


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """
    configuration = config.get_section(config.config_ini_section)
    configuration["sqlalchemy.url"] = get_url()
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            compare_type=True,
            include_schemas=True,
            process_revision_directives=process_revision_directives
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()